<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Thermodynamics of Intelligence • Shannon Labs</title>
  <meta name="description"
    content="Why we need Control Theory for AI. Connecting Shannon's Channel Capacity and Kolmogorov Complexity to the Shannon Control Unit.">

  <!-- IBM Plex Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;500;600;700&family=IBM+Plex+Mono:wght@400;500;600&family=IBM+Plex+Serif:ital,wght@0,400;0,600;1,400&display=swap"
    rel="stylesheet">

  <!-- Bell Labs Design System -->
  <link rel="stylesheet" href="bell-labs-design.css">

  <style>
    /* Manifesto-specific overrides */
    .manifesto-hero {
      background: linear-gradient(180deg, var(--white) 0%, var(--gray-50) 100%);
      padding: var(--space-4xl) var(--space-xl);
    }

    .manifesto-content {
      max-width: 800px;
      margin: 0 auto;
      font-family: var(--font-body);
      font-size: 1.125rem;
      line-height: 1.8;
    }

    .manifesto-content p {
      margin-bottom: var(--space-xl);
    }

    .manifesto-content h2 {
      font-family: var(--font-display);
      font-size: var(--text-2xl);
      margin-top: var(--space-3xl);
      margin-bottom: var(--space-lg);
      padding-bottom: var(--space-sm);
      border-bottom: 1px solid var(--gray-200);
    }

    .portrait-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: var(--space-xl);
      margin: var(--space-2xl) 0;
    }

    .portrait {
      text-align: center;
    }

    .portrait img {
      width: 150px;
      height: 150px;
      border-radius: 50%;
      border: 4px solid var(--white);
      box-shadow: var(--shadow-md);
      filter: grayscale(100%);
      transition: filter 0.3s ease;
      margin-bottom: var(--space-md);
    }

    .portrait:hover img {
      filter: grayscale(0%);
    }

    .portrait h3 {
      font-size: var(--text-lg);
      margin-bottom: var(--space-xs);
    }

    .portrait p {
      font-size: var(--text-sm);
      color: var(--gray-600);
      margin-bottom: 0;
    }

    .pull-quote {
      font-size: var(--text-xl);
      font-style: italic;
      color: var(--gray-800);
      border-left: 4px solid var(--primary);
      padding-left: var(--space-lg);
      margin: var(--space-2xl) 0;
    }
  </style>
</head>

<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <div class="grid-background"></div>

  <nav class="nav">
    <div class="nav-content">
      <a href="/" class="nav-logo">Shannon Labs</a>
      <div class="nav-links">
        <a href="/">Overview</a>
        <a href="research.html">Research</a>
        <a href="manifesto.html" class="is-active">Manifesto</a>
        <a href="https://huggingface.co/hunterbown/shannon-control-unit" target="_blank">Hugging Face</a>
        <a href="https://github.com/hmbown/shannon-control-unit" target="_blank">GitHub</a>
      </div>
      <button class="nav-menu-btn" aria-label="Toggle menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>

  <main id="main">
    <section class="manifesto-hero text-center">
      <div class="container">
        <h1 class="hero-title">The Thermodynamics of <span class="highlight">Intelligence</span></h1>
        <p class="hero-subtitle">Why we need Control Theory to bridge the gap between Shannon and Kolmogorov.</p>
      </div>
    </section>

    <article class="manifesto-content">
      <p class="lead">
        We are building AI systems that are increasingly powerful, yet we drive them like cars without speedometers. We
        guess at hyperparameters, we stare at loss curves, and we hope for convergence.
      </p>

      <p>
        This open-loop approach is unsustainable. To build the next generation of reliable, efficient AI, we must return
        to the first principles of information theory.
      </p>

      <div class="portrait-grid">
        <div class="portrait">
          <img src="photos/claude_shannon.jpg" alt="Claude Shannon">
          <h3>Claude Shannon</h3>
          <p>The Limit of Transmission</p>
        </div>
        <div class="portrait">
          <div
            style="width: 150px; height: 150px; background: var(--gray-100); border-radius: 50%; margin: 0 auto var(--space-md); display: flex; align-items: center; justify-content: center; border: 4px solid var(--white); box-shadow: var(--shadow-md);">
            <span style="font-family: var(--font-mono); font-size: 2rem; color: var(--gray-400);">K</span>
          </div>
          <h3>Andrey Kolmogorov</h3>
          <p>The Limit of Compression</p>
        </div>
      </div>

      <h2>The Two Limits</h2>
      <p>
        <strong>Claude Shannon</strong> taught us that reliable communication is limited by channel capacity. In the
        context of AI, our "channel" is the training process—the flow of information from the dataset into the model's
        parameters.
      </p>
      <p>
        <strong>Andrey Kolmogorov</strong> taught us that the complexity of an object is the length of the shortest
        program that can reproduce it. Intelligence, effectively, is compression. A model that has truly "learned" a
        dataset has found a compressed representation of it.
      </p>
      <p>
        The tension between these two is where learning happens. If a model compresses too little, it memorizes
        (overfitting). If it compresses too much, it fails to capture the nuance (underfitting).
      </p>

      <div class="pull-quote">
        "Intelligence isn't accumulating facts; it's compression efficiency."
      </div>

      <h2>The Missing Link: Control</h2>
      <p>
        Standard training schedules (fixed weight decay, cosine annealing) are <strong>open-loop</strong>. They assume a
        trajectory before the training even begins. But the "thermodynamics" of training—the rate at which a model
        absorbs information—changes dynamically.
      </p>
      <p>
        This is where <strong>Control Theory</strong> enters.
      </p>
      <p>
        The Shannon Control Unit (SCU) is our answer. By treating the <strong>Information Ratio</strong> (a metric
        derived from MDL) as a controlled variable, we can close the loop. We don't just guess a regularization
        schedule; we <em>govern</em> the model's complexity in real-time.
      </p>

      <h2>The Legacy</h2>
      <p>
        My great-grandfather, Ralph Bown Sr., was the Director of Research at Bell Labs when the transistor was
        invented. He understood that to scale a technology, you first need to control the fundamental physics.
      </p>
      <p>
        The transistor allowed us to control the flow of electrons with precision. The Shannon Control Unit allows us to
        control the flow of <em>information</em> with that same precision.
      </p>
      <p>
        We are not just training models. We are engineering the thermodynamics of intelligence.
      </p>

      <div class="text-center mt-5">
        <a href="research.html" class="btn btn-primary">See the Research</a>
      </div>
    </article>

    <footer class="footer">
      <div class="footer-content">
        <div class="footer-bottom">
          &copy; 2025 Shannon Labs.
        </div>
      </div>
    </footer>
  </main>

  <script>
    const menuBtn = document.querySelector('.nav-menu-btn');
    const navLinks = document.querySelector('.nav-links');
    menuBtn.addEventListener('click', () => {
      navLinks.classList.toggle('is-open');
      menuBtn.classList.toggle('is-active');
    });
  </script>
</body>

</html>