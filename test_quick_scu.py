#!/usr/bin/env python3
"""Quick test of SCU functionality"""

import torch
import math
from pathlib import Path
import sys
sys.path.append('.')

from configs.granite_1b_scu_config import create_memory_efficient_config
from scu.control import update_lambda

def test_scu_control():
    """Test the core SCU control logic"""
    print("ðŸ§ª Testing SCU control logic...")
    
    config = create_memory_efficient_config()
    print(f"Target S ratio: {config.target_s_ratio:.3%}")
    print(f"PI gains: Kp={config.kp}, Ki={config.ki}")
    
    # Simulate training steps
    lambda_current = config.lambda_init
    integral_term = 0.0
    
    print("\nSimulating training with SCU control:")
    for step in range(20):
        # Simulate varying S ratios (starting high, converging to target)
        if step < 5:
            s_ratio = 0.15 - step * 0.02  # High initial S ratio
        elif step < 10:
            s_ratio = 0.05 - (step - 5) * 0.005  # Converging
        else:
            s_ratio = config.target_s_ratio + (0.01 if step % 2 == 0 else -0.01)  # Around target
        
        # Apply SCU control (every 5 steps)
        if step % config.control_frequency == 0:
            lambda_current, integral_term, error = update_lambda(
                lambda_current, s_ratio, config.target_s_ratio, integral_term,
                Kp=config.kp, Ki=config.ki
            )
            
            # Apply bounds
            lambda_current = max(config.lambda_min, min(config.lambda_max, lambda_current))
            
            print(f"Step {step:2d}: S={s_ratio:.3f}, Î»={lambda_current:.3f}, error={error:+.3f}")
        else:
            print(f"Step {step:2d}: S={s_ratio:.3f}, Î»={lambda_current:.3f} (no control)")
    
    print("\nâœ… SCU control test completed successfully!")

def test_memory_efficiency():
    """Test memory usage patterns"""
    print("\nðŸ§  Testing memory efficiency...")
    
    # Simulate parameter BPT calculation
    mock_lora_params = torch.randn(81920)  # ~80K LoRA parameters
    prior_sigma = 0.01
    tokens_per_epoch = 100000
    
    param_sum = (mock_lora_params ** 2).sum()
    param_bpt = param_sum / (2 * prior_sigma**2 * tokens_per_epoch * math.log(2))
    
    print(f"Mock LoRA parameters: {len(mock_lora_params):,}")
    print(f"Parameter sum of squares: {param_sum:.2f}")
    print(f"ParamBPT: {param_bpt:.6f} bits/token")
    
    # Simulate data BPT
    mock_loss_nats = torch.tensor(2.5)  # Typical loss value
    data_bpt = mock_loss_nats / math.log(2)
    
    print(f"DataBPT: {data_bpt:.6f} bits/token")
    
    # Calculate S ratio
    total_bpt = data_bpt + param_bpt
    s_ratio = param_bpt / total_bpt
    
    print(f"Total BPT: {total_bpt:.6f} bits/token")
    print(f"S ratio: {s_ratio:.4f} ({s_ratio*100:.2f}%)")
    
    print("\nâœ… Memory efficiency test completed!")

if __name__ == "__main__":
    print("ðŸ”¬ Quick SCU Functionality Test")
    print("=" * 40)
    
    test_scu_control()
    test_memory_efficiency()
    
    print("\nðŸŽ‰ All tests passed! SCU is ready for training.")